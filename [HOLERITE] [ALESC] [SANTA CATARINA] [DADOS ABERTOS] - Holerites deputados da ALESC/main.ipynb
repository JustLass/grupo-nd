{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7bc0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8db3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://transparencia.alesc.sc.gov.br/deputados.php?ano=2025&mes=9'\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "tabela = soup.find('table', class_='pagamentos')\n",
    "df = pd.read_html(StringIO(str(tabela)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b45f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando extração de 70 meses com 3 workers e pausa randômica entre 2.0s e 5.0s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extraindo Dados por Mês: 100%|██████████| 70/70 [02:52<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "✅ EXTRAÇÃO COMPLETA E BEM-SUCEDIDA\n",
      "Total de registros obtidos: 2961\n",
      "Primeiras 5 linhas do DataFrame final:\n",
      "             Nome Subsídio (R$) Licença (Ato Plenário) Imposto de Renda  \\\n",
      "0     ADA DE LUCA      25322.25                      -          -5909.7   \n",
      "1    ALTAIR SILVA      25322.25                      -          -5909.7   \n",
      "2  ANA CAMPAGNOLO      25322.25                      -          -5909.7   \n",
      "3        PAULINHA      25322.25                      -          -5909.7   \n",
      "4     BRUNO SOUZA      25322.25                      -          -5909.7   \n",
      "\n",
      "  Contribuição Previdênciaria Referência   Ano Mês  \n",
      "0                     -671.11    01/2020  2020   1  \n",
      "1                     -671.11    01/2020  2020   1  \n",
      "2                     -671.11    01/2020  2020   1  \n",
      "3                     -671.11    01/2020  2020   1  \n",
      "4                     -671.11    01/2020  2020   1  \n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "# --- CONFIGURAÇÕES DE SEGURANÇA ---\n",
    "MAX_WORKERS = 3           # Reduz o número de requisições simultâneas\n",
    "TIMEOUT_SECS = 30         # Aumenta o tempo de espera antes de desistir\n",
    "PAUSA_MIN = 2.0           # Pausa mínima (segundos)\n",
    "PAUSA_MAX = 5.0           # Pausa máxima (segundos)\n",
    "MAX_TENTATIVAS = 3        # Número de repetições em caso de falha temporária\n",
    "# ---------------------------------\n",
    "\n",
    "def extrair_tabela(url):\n",
    "    \"\"\"\n",
    "    Extrai a tabela de uma única URL com lógica de repetição e pausas.\n",
    "    \n",
    "    Retorna:\n",
    "    - DataFrame em caso de sucesso.\n",
    "    - String de erro em caso de falha.\n",
    "    \"\"\"\n",
    "    for tentativa in range(1, MAX_TENTATIVAS + 1):\n",
    "        # 1. Pausa Randômica (Anti-Banimento)\n",
    "        pausa = random.uniform(PAUSA_MIN, PAUSA_MAX)\n",
    "        time.sleep(pausa) \n",
    "        \n",
    "        try:\n",
    "            # 2. Requisição com Timeout Estendido\n",
    "            response = requests.get(url, timeout=TIMEOUT_SECS)\n",
    "            \n",
    "            # 3. Verifica o Status Code\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                tabela = soup.find('table', class_='pagamentos')\n",
    "\n",
    "                if tabela:\n",
    "                    df = pd.read_html(StringIO(str(tabela)), thousands='.', decimal=',')[0]\n",
    "                    \n",
    "                    # Adiciona as colunas 'Ano' e 'Mês' para referência\n",
    "                    params = url.split('?')[-1].split('&')\n",
    "                    # Tenta extrair Ano e Mês, lidando com possíveis erros na URL\n",
    "                    try:\n",
    "                        ano = params[0].split('=')[-1]\n",
    "                        mes = params[1].split('=')[-1]\n",
    "                        df['Ano'] = ano\n",
    "                        df['Mês'] = mes\n",
    "                    except IndexError:\n",
    "                        df['Ano'] = 'Desconhecido'\n",
    "                        df['Mês'] = 'Desconhecido'\n",
    "                    \n",
    "                    return df # Retorna o DataFrame em caso de sucesso\n",
    "                else:\n",
    "                    return f\"Tabela não encontrada: {url}\"\n",
    "            \n",
    "            # Trata erros HTTP que não são Timeout (Ex: 404, 500)\n",
    "            else:\n",
    "                return f\"ERRO HTTP {response.status_code} na tentativa {tentativa}/{MAX_TENTATIVAS}: {url}\"\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            # Trata o erro específico de Timeout\n",
    "            if tentativa < MAX_TENTATIVAS:\n",
    "                # Usa tqdm.write para logar a repetição sem quebrar a barra de progresso\n",
    "                tqdm.write(f\"⏳ Timeout na URL. Tentando novamente ({tentativa + 1}/{MAX_TENTATIVAS})...\")\n",
    "                time.sleep(5) # Pausa fixa maior antes de tentar de novo\n",
    "                continue\n",
    "            else:\n",
    "                return f\"FALHA FINAL CONEXÃO (Timeout): {url}\"\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Trata outros erros de Requisição (Ex: DNS, SSL)\n",
    "            return f\"FALHA GERAL CONEXÃO: {e} - {url}\"\n",
    "\n",
    "    return None # Nunca deve ser alcançado, mas garante que a função retorna algo\n",
    "\n",
    "\n",
    "# --- GERAÇÃO DA LISTA DE URLs (Janeiro/2020 a Outubro/2025) ---\n",
    "\n",
    "anos = range(2020, 2026)\n",
    "meses = range(1, 13)\n",
    "base_url = 'https://transparencia.alesc.sc.gov.br/deputados.php?'\n",
    "urls_a_processar = []\n",
    "\n",
    "for ano, mes in product(anos, meses):\n",
    "    # Intervalo: de Jan/2020 até Out/2025\n",
    "    if ano == 2025 and mes > 10:\n",
    "        continue\n",
    "    \n",
    "    url_final = f'{base_url}ano={ano}&mes={mes}'\n",
    "    urls_a_processar.append(url_final)\n",
    "\n",
    "# --- EXECUÇÃO PARALELA COM TQDM ---\n",
    "\n",
    "resultados = []\n",
    "erros = []\n",
    "\n",
    "print(f\"Iniciando extração de {len(urls_a_processar)} meses com {MAX_WORKERS} workers e pausa randômica entre {PAUSA_MIN}s e {PAUSA_MAX}s.\")\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Mapeia a função para as URLs\n",
    "    futures = [executor.submit(extrair_tabela, url) for url in urls_a_processar]\n",
    "    \n",
    "    # Itera sobre os resultados com tqdm\n",
    "    for future in tqdm(futures, total=len(urls_a_processar), desc=\"Extraindo Dados por Mês\"):\n",
    "        resultado = future.result()\n",
    "        \n",
    "        if isinstance(resultado, pd.DataFrame):\n",
    "            resultados.append(resultado)\n",
    "        else:\n",
    "            # Se não for DataFrame, é uma mensagem de erro ou URL de falha\n",
    "            erros.append(resultado)\n",
    "            tqdm.write(f\"❌ Falha: {resultado}\") \n",
    "\n",
    "# --- COMBINAÇÃO FINAL ---\n",
    "\n",
    "if resultados:\n",
    "    # 4. Combina todos os DataFrames em um único resultado\n",
    "    df_final = pd.concat(resultados, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"✅ EXTRAÇÃO COMPLETA E BEM-SUCEDIDA\")\n",
    "    print(f\"Total de registros obtidos: {len(df_final)}\")\n",
    "    print(\"Primeiras 5 linhas do DataFrame final:\")\n",
    "    print(df_final.head())\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(\"\\n❌ Nenhuma tabela foi extraída com sucesso.\")\n",
    "\n",
    "if erros:\n",
    "    print(f\"\\n⚠️ Total de erros ({len(erros)}):\")\n",
    "    # Imprime os 5 primeiros erros para ter uma ideia\n",
    "    for i, erro in enumerate(erros[:5]):\n",
    "        print(f\"  - {erro}\")\n",
    "    if len(erros) > 5:\n",
    "        print(f\"  ...e mais {len(erros) - 5} erros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b5a4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "caminho = os.path.join('data', 'holerite_deputados_sc.csv')\n",
    "df_final.to_csv(caminho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
